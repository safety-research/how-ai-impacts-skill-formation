{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OoH7i8wTtwdY"
      },
      "source": [
        "## Task 1: Overcoming Distribution Shift for Better Predictors\n",
        "\n",
        "**Goal**: We have a dataset fitting a predictor on the entire dataset results in mediocre accuracy. We want to identify which feature to split the dataset on such that we produce two separate specialized predictors.\n",
        "\n",
        "**Learning Outcomes**: Learn about linear predictors. Apply linear predictors to find how to separate a dataset into two parts in order to indentify distribution shift.\n",
        "\n",
        "**Prerequisites**: Basic understanding of python and linear functions (e.g. y=mx+b).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 1: Creating a simple logistic regression predictor\n",
        "Given a dataset of two classes: class 0 (red circles) and class 1 (blue crosses), you can learn a classifier to classify the red circles and blue crosses into different categories. \n",
        "\n",
        "In this example, we'll look at two different predictors:\n",
        "1. A \"good\" predictor using logistic regression that learns from the data patterns\n",
        "2. A \"bad\" predictor that simply uses a vertical line to separate classes\n",
        "\n",
        "\n",
        "The visualization shows:\n",
        "- Left plot: The raw data points colored by their class\n",
        "- Middle plot: How the good predictor (logistic regression) separates the classes\n",
        "- Right plot: How the bad predictor (vertical line) attempts to separate the classes\n",
        "\n",
        "Run the cell below to see an interactive visualization. Click the \"Run Analysis\" \n",
        "button to generate new random data and see how the predictors perform.\n",
        "\n",
        "To make make a predictor, you can use the sklearn logistic regression function: \n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "X = np.vstack([np.random.randn(50,2) - 2, np.random.randn(50,2) + 2])\n",
        "y = np.hstack([np.zeros(50), np.ones(50)])\n",
        "\n",
        "# instantiate a logistic regression classifier\n",
        "good_clf = LogisticRegression()\n",
        "# fit the classifier to data \n",
        "good_clf.fit(X, y)\n",
        "\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "45582003cd59414fa3ff9fe4563c028e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Button(description='Run Analysis', style=ButtonStyle())"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import numpy as np\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import display\n",
        "import pandas as pd\n",
        "import ipywidgets as widgets\n",
        "\n",
        "def run_prediction_analysis():\n",
        "    # Generate a random binary classification dataset\n",
        "    np.random.seed(42)\n",
        "    X = np.vstack([np.random.randn(50,2) - 2, np.random.randn(50,2) + 2])\n",
        "    y = np.hstack([np.zeros(50), np.ones(50)])\n",
        "    y[np.random.choice(100, 3)] = 1 - y[np.random.choice(100, 3)]  # Flip ~3% of labels\n",
        "\n",
        "    # Create a figure with three subplots\n",
        "    fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(16, 4))\n",
        "\n",
        "    # First subplot: Show the two data classes\n",
        "    ax1.scatter(X[y==0, 0], X[y==0, 1], label='Class 0', alpha=0.8, c='red', marker='o')\n",
        "    ax1.scatter(X[y==1, 0], X[y==1, 1], label='Class 1', alpha=0.8, c='blue', marker='x')\n",
        "    ax1.set_xlabel('Feature 1')\n",
        "    ax1.set_ylabel('Feature 2')\n",
        "    ax1.set_title('Data Points by Class')\n",
        "    ax1.legend()\n",
        "\n",
        "    # Create a good predictor (logistic regression)\n",
        "    good_clf = LogisticRegression()\n",
        "    good_clf.fit(X, y)\n",
        "\n",
        "    # Create a deliberately bad predictor (vertical line at x=thresh)\n",
        "    thresh = -3\n",
        "    class BadPredictor:\n",
        "        def predict(self, X):\n",
        "            return X[:, 0] > thresh\n",
        "        \n",
        "        def score(self, X, y):\n",
        "            return np.mean(self.predict(X) == y)\n",
        "            \n",
        "    bad_clf = BadPredictor()\n",
        "\n",
        "    # Create mesh grid for plotting\n",
        "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
        "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
        "    xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1),\n",
        "                         np.arange(y_min, y_max, 0.1))\n",
        "\n",
        "    # Second subplot: Plot good predictor\n",
        "    Z = good_clf.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)\n",
        "    ax2.contourf(xx, yy, Z, alpha=0.4, cmap='RdYlBu')\n",
        "    ax2.scatter(X[y==0, 0], X[y==0, 1], label='Class 0', alpha=0.8, c='red', marker='o')\n",
        "    ax2.scatter(X[y==1, 0], X[y==1, 1], label='Class 1', alpha=0.8, c='blue', marker='x')\n",
        "    ax2.set_xlabel('Feature 1')\n",
        "    ax2.set_ylabel('Feature 2')\n",
        "    ax2.set_title(f'Good Predictor (Accuracy: {good_clf.score(X, y):.2f})')\n",
        "    # Plot decision boundary\n",
        "    Z_contour = good_clf.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:, 1].reshape(xx.shape)\n",
        "    ax2.contour(xx, yy, Z_contour, levels=[0.5], colors='k', linestyles='-')\n",
        "\n",
        "    # Third subplot: Plot bad predictor\n",
        "    Z = bad_clf.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape).astype(int)\n",
        "    ax3.contourf(xx, yy, Z, alpha=0.4, cmap='RdYlBu')\n",
        "    ax3.scatter(X[y==0, 0], X[y==0, 1], label='Class 0', alpha=0.8, c='red', marker='o')\n",
        "    ax3.scatter(X[y==1, 0], X[y==1, 1], label='Class 1', alpha=0.8, c='blue', marker='x')\n",
        "    ax3.set_xlabel('Feature 1')\n",
        "    ax3.set_ylabel('Feature 2')\n",
        "    ax3.set_title(f'Bad Predictor (Accuracy: {bad_clf.score(X, y):.2f})')\n",
        "    # Plot decision boundary\n",
        "    ax3.axvline(x=thresh, color='k', linestyle='-')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "button = widgets.Button(description='Run Analysis')\n",
        "button.on_click(lambda x: run_prediction_analysis())\n",
        "display(button) \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Part 2: Working with the dataset\n",
        "In this section we will download the dataset that we want to work with, we will show you how to see the features and work with the features. \n",
        "\n",
        "Using the ```ucimlrepo``` library, we can can download the dataset and examine the features. This dataset has 13 features and a target variable where the goal is to predict whether an individual has an income of above or below $50,000 USD. This data is drawn from the US Census. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "from folktables import ACSDataSource, ACSIncome\n",
        "  \n",
        "data_source = ACSDataSource(survey_year='2018', horizon='1-Year', survey='person')\n",
        "data = data_source.get_data(states=[\"RI\"], download=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Features: ['AGEP', 'COW', 'SCHL', 'MAR', 'OCCP', 'POBP', 'RELP', 'WKHP', 'SEX', 'RAC1P']\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Description</th>\n",
              "      <th>Data Type</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>AGEP</th>\n",
              "      <td>Age</td>\n",
              "      <td>Integer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>COW</th>\n",
              "      <td>Categorical</td>\n",
              "      <td>Class of Worker</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>SCHL</th>\n",
              "      <td>Educational Level</td>\n",
              "      <td>Integer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>MAR</th>\n",
              "      <td>Categorical</td>\n",
              "      <td>Marital Status</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>OCCP</th>\n",
              "      <td>Categorical</td>\n",
              "      <td>Occupation</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>POBP</th>\n",
              "      <td>Categorical</td>\n",
              "      <td>Place of Birth</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>RELP</th>\n",
              "      <td>Relationship</td>\n",
              "      <td>Categorical</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>WKHP</th>\n",
              "      <td>Usual Hours Worked per Week</td>\n",
              "      <td>Integer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>SEX</th>\n",
              "      <td>Binary</td>\n",
              "      <td>Sex</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>RAC1P</th>\n",
              "      <td>Race</td>\n",
              "      <td>Categorical</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                       Description        Data Type\n",
              "AGEP                           Age          Integer\n",
              "COW                    Categorical  Class of Worker\n",
              "SCHL             Educational Level          Integer\n",
              "MAR                    Categorical   Marital Status\n",
              "OCCP                   Categorical       Occupation\n",
              "POBP                   Categorical   Place of Birth\n",
              "RELP                  Relationship      Categorical\n",
              "WKHP   Usual Hours Worked per Week          Integer\n",
              "SEX                         Binary              Sex\n",
              "RAC1P                         Race      Categorical"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X, Y, _ = ACSIncome.df_to_numpy(data)\n",
        "\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
        "\n",
        "print(\"Features:\", ACSIncome.features)\n",
        "features_descriptions = {\n",
        "        'AGEP': {'Age', 'Integer'},\n",
        "        'COW': {'Class of Worker', 'Categorical'},\n",
        "        'SCHL': {'Educational Level', 'Integer'},\n",
        "        'MAR': {'Marital Status', 'Categorical'},\n",
        "        'OCCP': {'Occupation', 'Categorical'},\n",
        "        'POBP': {'Place of Birth', 'Categorical'},\n",
        "        'RELP': {'Relationship', 'Categorical'},\n",
        "        'WKHP': {'Usual Hours Worked per Week', 'Integer'},\n",
        "        'SEX': {'Sex', 'Binary'},\n",
        "        'RAC1P': {'Race', 'Categorical'},\n",
        "}\n",
        "pd.DataFrame.from_dict(features_descriptions, orient='index', columns=['Description', 'Data Type'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Part 3: Your task\n",
        "Your task is to split data into two groups and fit a predictor on to each group. You want to split the data in a way such that the average accuracy across all samples is as high as possible."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TASK ONLY \n",
        "features = ACSIncome.features\n",
        "\n",
        "# overall accuracy\n",
        "clf = LogisticRegression(max_iter=10000)\n",
        "clf.fit(X_train, Y_train)\n",
        "overall_score = clf.score(X_test, Y_test)\n",
        "print(f\"Overall Test Accuracy: {overall_score:.3f}\")\n",
        "\n",
        "def feature_mask(train_data:np.ndarray, feature_ind:int, threshold: float|None= None, equal:bool|None = None) -> np.ndarray:\n",
        "    if threshold is None and equal is None:\n",
        "        raise ValueError(\"Either threshold or equal must be provided\")\n",
        "    elif threshold is None:\n",
        "        return train_data[:, feature_ind] == equal\n",
        "    elif equal is None:\n",
        "        return train_data[:, feature_ind] < threshold\n",
        "    \n",
        "\n",
        "### YOUR CODE STARTS HERE \n",
        "# Your task is to split data into two groups and fit a predictor on to each group. You want to split the data in a way such that the average accuracy across all samples is as high as possible.\n",
        "# Step 1: Split the data into two groups\n",
        "\n",
        "\n",
        "# Step 2: Fit a predictor on each subgroup\n",
        "\n",
        "\n",
        "# Step 3: Evaluate the predictor\n",
        "\n",
        "# Step 4: Evaluate the predictor on the test dataset\n",
        "# Split test data using the same feature and threshold\n",
        "\n",
        "# Get scores on test data\n",
        "\n",
        "\n",
        "### YOUR CODE ENDS HERE ### \n",
        "\n",
        "# Calculate weighted average test score\n",
        "test_weighted_score = (test_score1 * len(X1_test) + test_score2 * len(X2_test)) / (len(X1_test) + len(X2_test))\n",
        "print(f\"Weighted Average Test Accuracy: {test_weighted_score:.3f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overall Accuracy: 0.802\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "Found array with 0 sample(s) (shape=(0, 10)) while a minimum of 1 is required by LogisticRegression.",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 38\u001b[39m\n\u001b[32m     35\u001b[39m clf1 = LogisticRegression(max_iter=\u001b[32m10000\u001b[39m)\n\u001b[32m     36\u001b[39m clf2 = LogisticRegression(max_iter=\u001b[32m10000\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m38\u001b[39m \u001b[43mclf1\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX1_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY1_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     39\u001b[39m clf2.fit(X2_train, Y2_train)\n\u001b[32m     41\u001b[39m \u001b[38;5;66;03m# Step 3: Evaluate the predictor\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Code/AFP/skill-dev/.venv/lib/python3.13/site-packages/sklearn/base.py:1389\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1382\u001b[39m     estimator._validate_params()\n\u001b[32m   1384\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1385\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1386\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1387\u001b[39m     )\n\u001b[32m   1388\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1389\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Code/AFP/skill-dev/.venv/lib/python3.13/site-packages/sklearn/linear_model/_logistic.py:1222\u001b[39m, in \u001b[36mLogisticRegression.fit\u001b[39m\u001b[34m(self, X, y, sample_weight)\u001b[39m\n\u001b[32m   1219\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1220\u001b[39m     _dtype = [np.float64, np.float32]\n\u001b[32m-> \u001b[39m\u001b[32m1222\u001b[39m X, y = \u001b[43mvalidate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1223\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1224\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1225\u001b[39m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1226\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcsr\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1227\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1228\u001b[39m \u001b[43m    \u001b[49m\u001b[43morder\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mC\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1229\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccept_large_sparse\u001b[49m\u001b[43m=\u001b[49m\u001b[43msolver\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mliblinear\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msag\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msaga\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1230\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1231\u001b[39m check_classification_targets(y)\n\u001b[32m   1232\u001b[39m \u001b[38;5;28mself\u001b[39m.classes_ = np.unique(y)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Code/AFP/skill-dev/.venv/lib/python3.13/site-packages/sklearn/utils/validation.py:2961\u001b[39m, in \u001b[36mvalidate_data\u001b[39m\u001b[34m(_estimator, X, y, reset, validate_separately, skip_check_array, **check_params)\u001b[39m\n\u001b[32m   2959\u001b[39m         y = check_array(y, input_name=\u001b[33m\"\u001b[39m\u001b[33my\u001b[39m\u001b[33m\"\u001b[39m, **check_y_params)\n\u001b[32m   2960\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2961\u001b[39m         X, y = \u001b[43mcheck_X_y\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mcheck_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2962\u001b[39m     out = X, y\n\u001b[32m   2964\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params.get(\u001b[33m\"\u001b[39m\u001b[33mensure_2d\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Code/AFP/skill-dev/.venv/lib/python3.13/site-packages/sklearn/utils/validation.py:1370\u001b[39m, in \u001b[36mcheck_X_y\u001b[39m\u001b[34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[39m\n\u001b[32m   1364\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1365\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m requires y to be passed, but the target y is None\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1366\u001b[39m     )\n\u001b[32m   1368\u001b[39m ensure_all_finite = _deprecate_force_all_finite(force_all_finite, ensure_all_finite)\n\u001b[32m-> \u001b[39m\u001b[32m1370\u001b[39m X = \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1371\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1372\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[43m=\u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1373\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccept_large_sparse\u001b[49m\u001b[43m=\u001b[49m\u001b[43maccept_large_sparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1374\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1375\u001b[39m \u001b[43m    \u001b[49m\u001b[43morder\u001b[49m\u001b[43m=\u001b[49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1376\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1377\u001b[39m \u001b[43m    \u001b[49m\u001b[43mforce_writeable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce_writeable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1378\u001b[39m \u001b[43m    \u001b[49m\u001b[43mensure_all_finite\u001b[49m\u001b[43m=\u001b[49m\u001b[43mensure_all_finite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1379\u001b[39m \u001b[43m    \u001b[49m\u001b[43mensure_2d\u001b[49m\u001b[43m=\u001b[49m\u001b[43mensure_2d\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1380\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_nd\u001b[49m\u001b[43m=\u001b[49m\u001b[43mallow_nd\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1381\u001b[39m \u001b[43m    \u001b[49m\u001b[43mensure_min_samples\u001b[49m\u001b[43m=\u001b[49m\u001b[43mensure_min_samples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1382\u001b[39m \u001b[43m    \u001b[49m\u001b[43mensure_min_features\u001b[49m\u001b[43m=\u001b[49m\u001b[43mensure_min_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1383\u001b[39m \u001b[43m    \u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m=\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1384\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_name\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mX\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1385\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1387\u001b[39m y = _check_y(y, multi_output=multi_output, y_numeric=y_numeric, estimator=estimator)\n\u001b[32m   1389\u001b[39m check_consistent_length(X, y)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Code/AFP/skill-dev/.venv/lib/python3.13/site-packages/sklearn/utils/validation.py:1130\u001b[39m, in \u001b[36mcheck_array\u001b[39m\u001b[34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_non_negative, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[39m\n\u001b[32m   1128\u001b[39m     n_samples = _num_samples(array)\n\u001b[32m   1129\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m n_samples < ensure_min_samples:\n\u001b[32m-> \u001b[39m\u001b[32m1130\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1131\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mFound array with \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[33m sample(s) (shape=\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m) while a\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1132\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33m minimum of \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[33m is required\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1133\u001b[39m             % (n_samples, array.shape, ensure_min_samples, context)\n\u001b[32m   1134\u001b[39m         )\n\u001b[32m   1136\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ensure_min_features > \u001b[32m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m array.ndim == \u001b[32m2\u001b[39m:\n\u001b[32m   1137\u001b[39m     n_features = array.shape[\u001b[32m1\u001b[39m]\n",
            "\u001b[31mValueError\u001b[39m: Found array with 0 sample(s) (shape=(0, 10)) while a minimum of 1 is required by LogisticRegression."
          ]
        }
      ],
      "source": [
        "## CLAUDE CODE\n",
        "\n",
        "# TASK ONLY \n",
        "features = ACSIncome.features\n",
        "\n",
        "# overall accuracy\n",
        "clf = LogisticRegression(max_iter=10000)\n",
        "clf.fit(X_train, Y_train)\n",
        "overall_score = clf.score(X_test, Y_test)\n",
        "print(f\"Overall Accuracy: {overall_score:.3f}\")\n",
        "\n",
        "def feature_mask(train_data:np.ndarray, feature_ind:int, threshold: float|None= None, equal:bool|None = None) -> np.ndarray:\n",
        "    if threshold is None and equal is None:\n",
        "        raise ValueError(\"Either threshold or equal must be provided\")\n",
        "    elif threshold is None:\n",
        "        return train_data[:, feature_ind] == equal\n",
        "    elif equal is None:\n",
        "        return train_data[:, feature_ind] < threshold\n",
        "    \n",
        "\n",
        "### YOUR CODE STARTS HERE \n",
        "# Step 1: Split the data into two groups\n",
        "best_feature = None\n",
        "best_threshold = None\n",
        "best_score = 0\n",
        "\n",
        "for feature in range(X_train.shape[1]):\n",
        "    for threshold in np.unique(X_train[:, feature]):\n",
        "        mask = feature_mask(X_train, feature, threshold=threshold)\n",
        "        \n",
        "        X1_train, X2_train = X_train[mask], X_train[~mask]\n",
        "        Y1_train, Y2_train = Y_train[mask], Y_train[~mask]\n",
        "        \n",
        "        # Step 2: Fit a predictor on each subgroup\n",
        "        clf1 = LogisticRegression(max_iter=10000)\n",
        "        clf2 = LogisticRegression(max_iter=10000)\n",
        "        \n",
        "        clf1.fit(X1_train, Y1_train)\n",
        "        clf2.fit(X2_train, Y2_train)\n",
        "        \n",
        "        # Step 3: Evaluate the predictor\n",
        "        score1 = clf1.score(X1_train, Y1_train)\n",
        "        score2 = clf2.score(X2_train, Y2_train)\n",
        "        \n",
        "        weighted_score = (score1 * len(X1_train) + score2 * len(X2_train)) / len(X_train)\n",
        "        \n",
        "        if weighted_score > best_score:\n",
        "            best_score = weighted_score\n",
        "            best_feature = feature\n",
        "            best_threshold = threshold\n",
        "\n",
        "# Use the best feature and threshold found\n",
        "mask_train = feature_mask(X_train, best_feature, threshold=best_threshold)\n",
        "X1_train, X2_train = X_train[mask_train], X_train[~mask_train]\n",
        "Y1_train, Y2_train = Y_train[mask_train], Y_train[~mask_train]\n",
        "\n",
        "clf1 = LogisticRegression(max_iter=10000)\n",
        "clf2 = LogisticRegression(max_iter=10000)\n",
        "\n",
        "clf1.fit(X1_train, Y1_train)\n",
        "clf2.fit(X2_train, Y2_train)\n",
        "\n",
        "# Step 4: Evaluate the predictor on the test dataset\n",
        "# Split test data using the same feature and threshold\n",
        "mask_test = feature_mask(X_test, best_feature, threshold=best_threshold)\n",
        "X1_test, X2_test = X_test[mask_test], X_test[~mask_test]\n",
        "Y1_test, Y2_test = Y_test[mask_test], Y_test[~mask_test]\n",
        "\n",
        "# Get scores on test data\n",
        "test_score1 = clf1.score(X1_test, Y1_test)\n",
        "test_score2 = clf2.score(X2_test, Y2_test)\n",
        "\n",
        "\n",
        "### YOUR CODE ENDS HERE ### \n",
        "\n",
        "# Calculate weighted average test score\n",
        "test_weighted_score = (test_score1 * len(X1_test) + test_score2 * len(X2_test)) / (len(X1_test) + len(X2_test))\n",
        "print(f\"Weighted Average Test Accuracy: {test_weighted_score:.3f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overall Test Accuracy: 0.802\n",
            "Weighted Average Test Accuracy: 0.808\n"
          ]
        }
      ],
      "source": [
        "features = ACSIncome.features\n",
        "\n",
        "# overall accuracy\n",
        "clf = LogisticRegression(max_iter=10000)\n",
        "clf.fit(X_train, Y_train)\n",
        "overall_score = clf.score(X_test, Y_test)\n",
        "print(f\"Overall Test Accuracy: {overall_score:.3f}\")\n",
        "\n",
        "### YOUR CODE STARTS HERE \n",
        "# Step 1: Split the data into two groups\n",
        "split_feature = 'SEX'\n",
        "feature_ind = list(features).index(split_feature)\n",
        "\n",
        "def feature_mask(train_data:np.ndarray, feature_ind:int, threshold: float|None= None, equal:bool|None = None) -> np.ndarray:\n",
        "    if threshold is None and equal is None:\n",
        "        raise ValueError(\"Either threshold or equal must be provided\")\n",
        "    elif threshold is None:\n",
        "        return train_data[:, feature_ind] == equal\n",
        "    elif equal is None:\n",
        "        return train_data[:, feature_ind] < threshold\n",
        "\n",
        "mask = feature_mask(X_train, feature_ind, equal=1)\n",
        "X1_train = X_train[mask]\n",
        "Y1_train = Y_train[mask]\n",
        "X2_train = X_train[~mask]\n",
        "Y2_train = Y_train[~mask]\n",
        "\n",
        "# Step 2: Fit a predictor on each subgroup\n",
        "clf1 = LogisticRegression(max_iter=10000)\n",
        "clf1.fit(X1_train, Y1_train)\n",
        "\n",
        "clf2 = LogisticRegression(max_iter=10000)\n",
        "clf2.fit(X2_train, Y2_train)\n",
        "\n",
        "# Step 3: Evaluate the predictor\n",
        "score1 = clf1.score(X1_train, Y1_train)\n",
        "score2 = clf2.score(X2_train, Y2_train)\n",
        "\n",
        "# Step 4: Evaluate the predictor on the test dataset\n",
        "# Split test data using the same feature and threshold\n",
        "mask_test = feature_mask(X_test, feature_ind, equal=1)\n",
        "X1_test = X_test[mask_test]\n",
        "Y1_test = Y_test[mask_test]\n",
        "X2_test = X_test[~mask_test]\n",
        "Y2_test = Y_test[~mask_test]\n",
        "\n",
        "# Get scores on test data\n",
        "test_score1 = clf1.score(X1_test, Y1_test)\n",
        "test_score2 = clf2.score(X2_test, Y2_test)\n",
        "\n",
        "### YOUR CODE ENDS HERE ### \n",
        "\n",
        "# Calculate weighted average test score\n",
        "test_weighted_score = (test_score1 * len(X1_test) + test_score2 * len(X2_test)) / (len(X1_test) + len(X2_test))\n",
        "print(f\"Weighted Average Test Accuracy: {test_weighted_score:.3f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
