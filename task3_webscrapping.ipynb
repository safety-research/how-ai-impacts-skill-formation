{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E9tYOP3mvGgL"
      },
      "source": [
        "## Task 3: Webscrapping with Beautiful Soup\n",
        "**Goal**: We will learn about scraping a webpage with beautiful soup\n",
        "\n",
        "**Learning Outcomes**: Learn to use beautiful soup to scape different websites. \n",
        "\n",
        "**Prerequisites**: Basic understanding of python."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Part 1: Introduction to Beautiful Soup\n",
        "Beautiful Soup is a Python library for pulling data out of HTML and XML files. It works with different parsers to provide ways of navigating, searching, and modifying the parse tree. It commonly saves programmers hours or days of work.\n",
        "\n",
        "Here is an example html doc: \n",
        "```python\n",
        "html_doc = \"\"\"\n",
        "<html><head><title>The Dormouse's story</title></head>\n",
        "<body>\n",
        "<p class=\"title\"><b>The Dormouse's story</b></p>\n",
        "\n",
        "<p class=\"story\">Once upon a time there were three little sisters; and their names were\n",
        "<a href=\"http://example.com/elsie\" class=\"sister\" id=\"link1\">Elsie</a>,\n",
        "<a href=\"http://example.com/lacie\" class=\"sister\" id=\"link2\">Lacie</a> and\n",
        "<a href=\"http://example.com/tillie\" class=\"sister\" id=\"link3\">Tillie</a>;\n",
        "and they lived at the bottom of a well.</p>\n",
        "\n",
        "<p class=\"story\">...</p>\n",
        "\"\"\"\n",
        "```\n",
        "\n",
        "You can import BeautifulSoup and create a BeautifulSoup object like follows: \n",
        "```python\n",
        "from bs4 import BeautifulSoup\n",
        "soup = BeautifulSoup(html_doc, 'html.parser')\n",
        "```\n",
        "\n",
        "Now you can try many differnt ways to navigate the BeauifulSoup data structure below: "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<title>The Dormouse's story</title>\n",
            "title\n",
            "The Dormouse's story\n",
            "head\n",
            "<p class=\"title\"><b>The Dormouse's story</b></p>\n",
            "['title']\n",
            "<a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\">Elsie</a>\n",
            "[<a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\">Elsie</a>, <a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\">Lacie</a>, <a class=\"sister\" href=\"http://example.com/tillie\" id=\"link3\">Tillie</a>]\n",
            "<a class=\"sister\" href=\"http://example.com/tillie\" id=\"link3\">Tillie</a>\n"
          ]
        }
      ],
      "source": [
        "from bs4 import BeautifulSoup\n",
        "\n",
        "html_doc = \"\"\"\n",
        "<html><head><title>The Dormouse's story</title></head>\n",
        "<body>\n",
        "<p class=\"title\"><b>The Dormouse's story</b></p>\n",
        "\n",
        "<p class=\"story\">Once upon a time there were three little sisters; and their names were\n",
        "<a href=\"http://example.com/elsie\" class=\"sister\" id=\"link1\">Elsie</a>,\n",
        "<a href=\"http://example.com/lacie\" class=\"sister\" id=\"link2\">Lacie</a> and\n",
        "<a href=\"http://example.com/tillie\" class=\"sister\" id=\"link3\">Tillie</a>;\n",
        "and they lived at the bottom of a well.</p>\n",
        "\n",
        "<p class=\"story\">...</p>\n",
        "\"\"\"\n",
        "\n",
        "soup = BeautifulSoup(html_doc, 'html.parser')\n",
        "\n",
        "print(soup.title)\n",
        "# <title>The Dormouse's story</title>\n",
        "\n",
        "print(soup.title.name)\n",
        "# u'title'\n",
        "\n",
        "print(soup.title.string)\n",
        "# u'The Dormouse's story'\n",
        "\n",
        "print(soup.title.parent.name)\n",
        "# u'head'\n",
        "\n",
        "print(soup.p)\n",
        "# <p class=\"title\"><b>The Dormouse's story</b></p>\n",
        "\n",
        "print(soup.p['class'])\n",
        "# u'title'\n",
        "\n",
        "print(soup.a)\n",
        "# <a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\">Elsie</a>\n",
        "\n",
        "print(soup.find_all('a'))\n",
        "# [<a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\">Elsie</a>,\n",
        "#  <a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\">Lacie</a>,\n",
        "#  <a class=\"sister\" href=\"http://example.com/tillie\" id=\"link3\">Tillie</a>]\n",
        "\n",
        "print(soup.find(id=\"link3\"))\n",
        "# <a class=\"sister\" href=\"http://example.com/tillie\" id=\"link3\">Tillie</a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Two common tasks are \n",
        "1. Extracting all the URLs \n",
        "2. Extracting all the text from a page\n",
        "\n",
        "See below for examples on how to do this: "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting all the URLs\n",
            "http://example.com/elsie\n",
            "http://example.com/lacie\n",
            "http://example.com/tillie\n",
            "Extracting all the text from a page\n",
            "\n",
            "The Dormouse's story\n",
            "\n",
            "The Dormouse's story\n",
            "Once upon a time there were three little sisters; and their names were\n",
            "Elsie,\n",
            "Lacie and\n",
            "Tillie;\n",
            "and they lived at the bottom of a well.\n",
            "...\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(\"Extracting all the URLs\")\n",
        "for link in soup.find_all('a'):\n",
        "    print(link.get('href'))\n",
        "\n",
        "print(\"Extracting all the text from a page\")\n",
        "print(soup.get_text())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Part 2: Scrapping from the Web \n",
        "Where do we get the html/xml pages from? Python's requests library handles HTTP communication by making HTTP requests to web servers (GET, POST, etc,). This library will help us retrieve raw HTML content from the website. When we encounter an error, this library handles network related errors. \n",
        "\n",
        "```python\n",
        "import requests\n",
        "response = requests.get('http://books.toscrape.com/')\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "GR-OmrHLvZWI"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "response = requests.get('http://books.toscrape.com/')\n",
        "\n",
        "soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "title = soup.title.text\n",
        "all_paragraphs = soup.find_all('p')\n",
        "specific_div = soup.find('div', class_='content')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Part 3: Build an inventory by scrapping a website\n",
        "\n",
        "Your task is to scrape a book website and collect the prices for all the books you find."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Price</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>unique</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>top</th>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>freq</th>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       Price\n",
              "count      0\n",
              "unique     0\n",
              "top      NaN\n",
              "freq     NaN"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# TASK ONLY\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "\n",
        "# This is the url of the website we will scrape\n",
        "base_url = 'http://books.toscrape.com/'\n",
        "book_inventory = {} # store you results in this dictionary here\n",
        "\n",
        "### YOUR CODE STARTS HERE\n",
        "# Your task is to scrape a book website and collect the prices for all the books you find. \n",
        "# make sure you store the results in the book_inventory dictionary with the price as a float\n",
        "# make sure you scrape all the pages so you have 1000 books in total\n",
        "\n",
        "\n",
        "### YOUR CODE ENDS HERE\n",
        "\n",
        "all_books_df = pd.DataFrame.from_dict(book_inventory, orient='index', columns=['Price']) # your book inventory should be a dictionary with the book title as the key and the price as the value\n",
        "all_books_df.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# CLAUDE CODE\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "\n",
        "# This is the url of the website we will scrape\n",
        "base_url = 'http://books.toscrape.com/'\n",
        "book_inventory = {} # store you results in this dictionary here\n",
        "\n",
        "### YOUR CODE STARTS HERE\n",
        "# Your task is to scrape a book website and collect the prices for all the books you find. \n",
        "# make sure you store the results in the book_inventory dictionary with the price as a float\n",
        "# make sure you scrape all the pages so you have 1000 books in total\n",
        "def scrape_page(url):\n",
        "    response = requests.get(url)\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "    books = soup.find_all('article', class_='product_pod')\n",
        "    \n",
        "    for book in books:\n",
        "        title = book.h3.a['title']\n",
        "        price = float(book.select_one('p.price_color').text.strip('£'))\n",
        "        book_inventory[title] = price\n",
        "\n",
        "def get_next_page(url):\n",
        "    response = requests.get(url)\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "    next_button = soup.select_one('li.next > a')\n",
        "    if next_button:\n",
        "        return base_url + next_button['href']\n",
        "    return None\n",
        "\n",
        "current_url = base_url + 'catalogue/page-1.html'\n",
        "\n",
        "while current_url and len(book_inventory) < 1000:\n",
        "    scrape_page(current_url)\n",
        "    current_url = get_next_page(current_url)\n",
        "\n",
        "### YOUR CODE ENDS HERE\n",
        "\n",
        "all_books_df = pd.DataFrame.from_dict(book_inventory, orient='index', columns=['Price']) # your book inventory should be a dictionary with the book title as the key and the price as the value\n",
        "all_books_df.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/jhs/Code/AFP/skill-dev/.venv/lib/python3.13/site-packages/bs4/__init__.py:339: UserWarning: You provided Unicode markup but also provided a value for from_encoding. Your from_encoding will be ignored.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Price</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>999.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>35.059389</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>14.449765</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>10.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>22.105000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>35.960000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>47.475000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>59.990000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "            Price\n",
              "count  999.000000\n",
              "mean    35.059389\n",
              "std     14.449765\n",
              "min     10.000000\n",
              "25%     22.105000\n",
              "50%     35.960000\n",
              "75%     47.475000\n",
              "max     59.990000"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# MY CODE\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# This is the url of the website we will scrape\n",
        "base_url = 'http://books.toscrape.com/'\n",
        "book_inventory = {} # store you results in this dictionary here\n",
        "\n",
        "### YOUR CODE STARTS HERE\n",
        "# Your task is to scrape a book website and collect the prices for all the books you find. \n",
        "# make sure you store the results in the book_inventory dictionary with the price as a float\n",
        "# make sure you scrape all the pages so you have 1000 books in total\n",
        "\n",
        "response = requests.get(base_url)\n",
        "soup = BeautifulSoup(response.text, 'html.parser', from_encoding='utf-8')\n",
        "NUM_PAGES=0 \n",
        "while True: \n",
        "    all_books = soup.find_all('article', class_='product_pod')\n",
        "    if len(all_books) != 20: \n",
        "        print(all_books)\n",
        "    for book in all_books:\n",
        "        title = book.find('h3').find('a')['title']\n",
        "        price = book.find('p', class_='price_color').text\n",
        "        book_inventory[title] = float(price.split('£')[1])\n",
        "    \n",
        "    next_page = soup.select_one('li.next')\n",
        "    if not next_page:\n",
        "        break\n",
        "    url = base_url + 'catalogue/' + next_page.find('a')['href'].split('/')[-1]\n",
        "    response = requests.get(url)\n",
        "    soup = BeautifulSoup(response.text, 'html.parser', from_encoding='utf-8')\n",
        "    NUM_PAGES+=1 \n",
        "### YOUR CODE ENDS HERE\n",
        "\n",
        "all_books_df = pd.DataFrame.from_dict(book_inventory, orient='index', columns=['Price']) # your book inventory should be a dictionary with the book title as the key and the price as the value\n",
        "all_books_df.describe()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
